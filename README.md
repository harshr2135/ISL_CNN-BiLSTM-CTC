
# ğŸ¤Ÿ Indian Sign Language Recognition (CNN + BiLSTM + CTC)

This project implements an end-to-end system for **continuous Indian Sign Language (ISL)** recognition using a hybrid deep learning model that combines:
- **CNN** for spatial feature extraction
- **BiLSTM** for sequence modeling
- **CTC Loss** for alignment-free sequence decoding

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_videos/              # Input videos organized in class-wise folders
â”‚   â””â”€â”€ processed_data/          # Preprocessed .npy frame sequences
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ isl_ctc_model.py         # CNN + BiLSTM model definition
â”‚   â””â”€â”€ ctc_trainer.py           # CTC wrapper for model training
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ video_loader.py          # Frame extractor from videos
â”‚   â”œâ”€â”€ label_utils.py           # Label map creation and loading
â”‚   â”œâ”€â”€ dataset_loader.py        # Loads dataset into tf.data pipeline
â”‚   â”œâ”€â”€ ctc_decoder.py           # Greedy decoding logic for CTC
â”‚   â””â”€â”€ metrics.py               # WER and accuracy computation
â”‚
â”œâ”€â”€ preprocess.py                # Converts raw videos to fixed-length .npy files
â”œâ”€â”€ train_model.py               # Loads dataset, builds model, trains and evaluates
â”œâ”€â”€ inference_realtime.py        # Captures webcam input and runs real-time inference
â”œâ”€â”€ label_map.json               # Word-to-label and label-to-word mappings
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Project documentation (this file)
```

---

## âœ… Key Features

- Processes `.mp4`, `.avi`, `.mov`, `.mkv` videos
- Frame-extraction, resizing, and normalization
- CNN-based spatial encoder
- BiLSTM for learning temporal dependencies
- CTC loss for variable-length gesture decoding
- Real-time webcam-based prediction
- Evaluation using WER and sequence accuracy

---

## âš™ï¸ Setup Instructions

### 1. Install Dependencies

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### 2. Prepare Dataset

Place raw videos into class-named folders like:

```
data/raw_videos/
â”œâ”€â”€ hello/
â”‚   â”œâ”€â”€ hello1.mp4
â”‚   â”œâ”€â”€ hello2.avi
â”‚   ...
â”œâ”€â”€ thank_you/
â”‚   â”œâ”€â”€ thank1.mp4
â”‚   ...
```

### 3. Preprocess Videos

```bash
python preprocess.py
```

> This saves frame sequences as `.npy` files in `data/processed_data/` and generates `label_map.json`.

---

## ğŸ‹ï¸ Model Training

```bash
python train_model.py
```

- Trains on 80% of the dataset, validates on 20%
- Automatically logs:
  - Best model in `models/`
  - TensorBoard logs in `logs/`
- Prints final:
  - âœ… Accuracy
  - ğŸ“ Word Error Rate (WER)

---

## ğŸ¥ Real-Time Inference

```bash
python inference_realtime.py
```

- Launches webcam
- Captures short sequence (up to 60 frames)
- Predicts sign sequence using the trained model
- Prints predicted phrase

> âš ï¸ Update the `MODEL_PATH` in `inference_realtime.py` with your actual `.keras` model file path.

---

## ğŸ“Š Evaluation Metrics

- âœ… **Exact Match Accuracy**: Number of correct full-sequence predictions
- ğŸ“ **Word Error Rate (WER)**: Levenshtein distance between predicted and true sequences
- ğŸ“ˆ **Top-k Accuracy** (optional extension)

---

## ğŸ“¦ Requirements

```
tensorflow>=2.11.0
opencv-python
numpy
tqdm
scikit-learn
```

Install via:

```bash
pip install -r requirements.txt
```

---

## ğŸ’¡ Future Improvements

- Beam search decoder for better predictions
- Face and hand landmark fusion
- Multi-signer generalization
- Export to TensorRT/ONNX for Jetson deployment

---

## ğŸ§  Skills Applied

- Temporal Deep Learning (CNN + BiLSTM)
- Sequence Modeling with CTC Loss
- Computer Vision (OpenCV)
- Real-Time Inference
- TensorFlow & tf.data pipeline
- Evaluation metrics like WER

---

## ğŸ“ License

MIT License. For academic and research use.

---

## ğŸ™ Acknowledgments

This system is part of a larger effort to build accessible communication tools for the deaf and hard-of-hearing community in India using state-of-the-art AI and computer vision techniques.
